# -*- coding: utf-8 -*-
"""4 VLms

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sFCESOgoe_lDA-IFAfAJ_Ju07aMJg2tn
"""

!pip install transformers
!pip install torchvision
!pip install timm
!pip install accelerate
!pip install Pillow

"""BLIP (Bootstrap Language Image Pretraining)"""

from google.colab import files
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import matplotlib.pyplot as plt

# Step 1: Upload multiple images
uploaded = files.upload()

# Step 2: Load model & processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to("cuda" if torch.cuda.is_available() else "cpu")

# Step 3: Loop through each uploaded image
for filename in uploaded.keys():
    print(f"\nüì∑ Processing: {filename}")
    raw_image = Image.open(filename).convert('RGB')

    # Display image
    plt.imshow(raw_image)
    plt.axis('off')
    plt.show()

    # Step 4: Preprocess and caption
    inputs = processor(images=raw_image, return_tensors="pt").to(model.device)
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)

    print("üñºÔ∏è Caption:")
    print(caption)

"""Git (Generative Image-to-Text Transformer)"""

from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForCausalLM
from google.colab import files
import matplotlib.pyplot as plt

# Load Git model & processor
model_name = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

# Upload multiple images
uploaded = files.upload()

# Generate captions
for filename in uploaded:
    print(f"\nüì∑ {filename}")
    image = Image.open(filename).convert("RGB")

    # Show image
    plt.imshow(image)
    plt.axis("off")
    plt.show()

    # Process and generate caption
    pixel_values = processor(images=image, return_tensors="pt").pixel_values.to(model.device)
    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print("üß† Caption:", caption)

"""SmolVLM"""

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
from google.colab import files
import matplotlib.pyplot as plt

# Set device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load processor and model
processor = AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM-256M-Instruct")
model = AutoModelForVision2Seq.from_pretrained("HuggingFaceTB/SmolVLM-256M-Instruct").to(device)

# Upload images
uploaded = files.upload()

for filename in uploaded.keys():
    print(f"\nüì∑ Processing: {filename}")
    image = Image.open(filename).convert("RGB")

    # Display image
    plt.imshow(image)
    plt.axis('off')
    plt.show()

    # Prepare inputs
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "tell me what exactly is in the pic."}
            ]
        }
    ]
    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=[image], return_tensors="pt").to(device)

    # Generate caption
    generated_ids = model.generate(**inputs, max_new_tokens=50)
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print(f"üß† Caption: {caption}")

"""NanoVLM"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/huggingface/nanoVLM.git
# %cd nanoVLM
!pip install torch numpy torchvision pillow datasets huggingface-hub transformers wandb
!pip install transformers torchvision accelerate
from models.vision_language_model import VisionLanguageModel
model = VisionLanguageModel.from_pretrained("lusxvr/nanoVLM-222M")

# 2. Upload your image
from google.colab import files
uploaded = files.upload()
image_path = next(iter(uploaded))

# 3. Import required modules
import torch
from PIL import Image
from models.vision_language_model import VisionLanguageModel
from data.processors import get_tokenizer, get_image_processor

# 4. Manually set arguments (replacing argparse)
checkpoint = None  # or path to local weights
hf_model = "lusxvr/nanoVLM-222M"
prompt = "What is this is food item?"
generations = 3
max_new_tokens = 30

# 5. Load device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 6. Load model
source = checkpoint if checkpoint else hf_model
model = VisionLanguageModel.from_pretrained(source).to(device).eval()

# 7. Load tokenizer and image processor
tokenizer = get_tokenizer(model.cfg.lm_tokenizer)
image_processor = get_image_processor(model.cfg.vit_img_size)

# 8. Encode prompt and image
template = f"Question: {prompt} Answer:"
tokens = tokenizer([template], return_tensors="pt")["input_ids"].to(device)
image = Image.open(image_path).convert("RGB")
img_tensor = image_processor(image).unsqueeze(0).to(device)

# 9. Generate text
print("\nüîç Prompt:", prompt)
print("üß† Generations:")
for i in range(generations):
    output = model.generate(tokens, img_tensor, max_new_tokens=max_new_tokens)
    decoded = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
    print(f"  ‚úÖ Generation {i+1}: {decoded}")